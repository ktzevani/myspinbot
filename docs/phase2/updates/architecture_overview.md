# üß≠ Phase 2 Architecture Overview

This document describes the **current architecture of MySpinBot as implemented in Phase 2**, and how it relates to the original end‚Äëstate design from the root `docs/` series. It focuses on the **dual‚Äëplane LangGraph orchestration**, shared schema/config system, and the way the existing codebase actually behaves today.

---

## 1Ô∏è‚É£ High‚ÄëLevel System Architecture (Phase 2 Snapshot)

At a high level the system is now a **three‚Äëtier stack**:

- **Control Plane (backend/)** ‚Äî Node.js + Fastify + LangGraph.js  
  Orchestrates workflows, exposes HTTP + WebSocket APIs, and persists job state in Redis.
- **Data Plane (worker/)** ‚Äî Python + FastAPI + LangGraph.py  
  Executes python‚Äëplane LangGraph nodes (GPU‚Äëstyle tasks), uploads artifacts to MinIO, and publishes progress via Redis Pub/Sub.
- **Experience Layer (frontend/)** ‚Äî Next.js UI for triggering training requests and monitoring job progress.
- **Infra Layer (infra/ + shared services)** ‚Äî Redis, MinIO, Traefik, Prometheus/Grafana, cAdvisor/DCGM.

### System Map (Current + Planned Targets)

```mermaid
flowchart LR
    %% Ingress & UI
    subgraph Ingress["Traefik Ingress"]
        T{{TLS & Routing}}
    end

    subgraph Frontend
        UI[Next.js Web App]
    end

    subgraph ControlPlane["Control Plane ‚Äî Node.js"]
        API[Fastify API + WS]
        PLNR["Planner (LangGraph.js)"]
        EXEC[Control Executor]
        JQ["JobQueue (Redis Streams + Pub/Sub)"]
    end

    subgraph DataPlane["Data Plane ‚Äî Python"]
        WAPI[FastAPI /metrics + /health]
        WEXEC["Worker Executor (LangGraph.py)"]
        WBR["Redis Bridge (Streams + Pub/Sub)"]
        TASKS["Task Registry(train_lora, train_voice, render_video, get_capabilities)"]
    end

    subgraph Data["State & Storage"]
        R[(Redis ‚Äî Streams + Pub/Sub)]
        S3[(MinIO / S3)]
        CONF[(Shared JSON Schemas + Config)]
    end

    subgraph Observability
        PR[Prometheus]
        GF[Grafana]
        CA[cAdvisor]
        NV[NVIDIA DCGM Exporter]
    end

    %% Planned external AI services (Phase 3+)
    subgraph AI_Plan["AI Services (Planned / Stubbed)"]
        CU[(ComfyUI Pipelines)]
        OL[(Ollama LLM)]
        TTS[(F5-TTS / GPT-SoVITS)]
        W2L[(Wav2Lip / SadTalker)]
    end

    %% Connections
    T --> UI
    T --> API
    UI <--> API

    API --> PLNR
    PLNR --> JQ
    API <--> JQ

    JQ <--> R
    R <--> WBR
    WBR <--> WEXEC
    WEXEC --> TASKS
    WEXEC --> WAPI

    TASKS --> S3

    PR --> GF
    PR <--> API
    PR <--> WAPI
    PR <--> CA
    PR <--> NV

    %% Planned integrations (not fully wired yet)
    PLNR -. future .-> OL
    WEXEC -. future .-> CU
    WEXEC -. future .-> TTS
    WEXEC -. future .-> W2L
    CU -. artifacts .-> S3
    TTS -. models .-> S3
```

**Implementation status (Phase 2):**

- **Fully implemented:** Fastify API, WS gateway, Planner, control‚Äëplane Executor, Redis JobQueue, Python Executor, task registry, MinIO artifact stubs, shared JSON schemas, codegen, metrics.
- **Partially implemented / stubbed:** Script generation (LLM), artifact management, GPU‚Äëspecific workloads (LoRA, video render) ‚Äî all simulated but wired.
- **Planned (Phase 3+):** Real LLM via Ollama, ComfyUI workflows, TTS + lip‚Äësync models, richer agentic planner.

---

## 2Ô∏è‚É£ Dual‚ÄëPlane LangGraph Execution

Phase 2 implements a concrete version of the **Dual‚ÄëPlane LangGraph Orchestration** described in `dual_orchestration.md`:

- A **LangGraph graph JSON** represents the full job: both control‚Äëplane and data‚Äëplane nodes.
- The **Control Plane** executes `plane: "node"` nodes (e.g. script generation, manifest merging).
- The **Data Plane** executes `plane: "python"` nodes (e.g. `train_lora`, `train_voice`, `render_video`), then hands the updated graph back.

### Execution Loop (Control ‚Üî Data)

```mermaid
sequenceDiagram
    autonumber
    participant UI as Next.js UI
    participant API as Fastify API
    participant PL as Planner (Node)
    participant CQ as Control Stream (Redis)
    participant CE as Control Executor
    participant DQ as Data Stream (Redis)
    participant WE as Worker Executor (Python)
    participant TK as Python Tasks

    UI->>API: POST /api/train
    API->>PL: build graph (script ‚Üí train_lora ‚Ä¶)
    PL-->>API: graph JSON (langgraph.v1)
    API->>CQ: enqueue control job (XADD)

    CE->>CQ: read pending graph
    CE->>CE: execute control-plane nodes
    CE->>DQ: if python nodes remain ‚Üí XADD to data stream

    WE->>DQ: read python-plane job
    WE->>TK: run python-plane node(s)
    TK-->>WE: progress callbacks, artifact URIs
    WE->>CQ: if control nodes remain ‚Üí enqueue updated graph
    WE-->>CQ: else publish completed/failed status

    CE->>CQ: resume control-plane nodes (if any)
    CE-->>API: final graph status
    API-->>UI: status via WebSocket (mirrored from Redis)
```

Key properties:

- **Graphs are the contract** ‚Äî they carry node definitions, plane assignments, and outputs.
- **Redis Streams** form the **control/data bridge**; Pub/Sub carries status and progress.
- **Executors are idempotent** ‚Äî they can resume a partially completed graph after restart.

---

## 3Ô∏è‚É£ Training & Capabilities Workflows (Current Behavior)

### A. Default Training Workflow

Today, the primary entrypoint is **`POST /api/train`**, which always creates a **default training graph** via the Planner:

- **Control‚Äëplane nodes** (Node.js)
  - `script.generateScript` ‚Äî stubbed script generator (future Ollama call).
  - `script.postProcess` (conceptual) ‚Äî any post‚Äëscript transformations.
- **Data‚Äëplane nodes** (Python)
  - `train_lora` ‚Äî simulates LoRA training, uploads dummy artifact to MinIO.
  - `train_voice` (planned) ‚Äî placeholder for TTS/voice fine‚Äëtuning.
  - `render_video` ‚Äî simulates video render, uploads dummy artifact.

Conceptually:

```mermaid
flowchart TD
    A[User POST /api/train] --> B[Planner<br/>control-plane]
    B --> C["Node LangGraph: script.generateScript"]
    C --> D["Python LangGraph: train_lora"]
    D --> E["Python LangGraph: render_video"]
    E --> F[Job Completed + Artifacts in MinIO]
```

The **graph shape** is intentionally generic so the Agentic Planner described in `dual_orchestration.md` can later synthesize richer graphs from prompts and capability manifests.

### B. Capabilities Workflow

`GET /api/capabilities` is implemented as a **two‚Äënode hybrid graph**:

1. Python node `get_capabilities` ‚Äî returns worker capability manifest.
2. Node node `capabilities.getManifest` ‚Äî merges worker manifest with control‚Äëplane capabilities into a single JSON object.

This is the first concrete use of **dual‚Äëplane graphs** wired end‚Äëto‚Äëend in code; future workflows will follow the same pattern.

---

## 4Ô∏è‚É£ Shared Schemas, Job State & WebSockets

Phase 2 establishes a **schema‚Äëdriven runtime**:

- Canonical JSON Schemas under `common/config/schemas/**` define:
  - LangGraph graph format
  - Job messaging and status
  - Redis bridge configuration
  - Capability manifests
- **Backend**: AJV validators (code‚Äëgenerated) enforce graph and config correctness.
- **Worker**: Pydantic models (code‚Äëgenerated) enforce the same contracts.

### Job State & WebSocket Flow

```mermaid
flowchart LR
    U[Next.js Client] -->|POST /api/train| A[Fastify API]
    A --> B[JobQueue<br/>persist job:* in Redis]
    B <--> F[Control Executor]
    B <--> C[Redis Streams & Pub/Sub]
    C <--> R["Redis Bridge (Worker)"]
    R <--> D[Worker Executor]
    C --> B
    B -->|poll job state| E[WebSocket Hub]
    E -->|"{type:'update', ...}"| U
```

- The **JobQueue** mirrors worker Pub/Sub events into **job:\<id\>** keys.
- The WS hub polls the JobQueue at `configuration.websocket.updateInterval` and pushes consolidated state to subscribers.
- Clients subscribe/unsubscribe **per jobId** and stop listening when the job is completed.

---

## 5Ô∏è‚É£ How This Aligns with the Original Architecture

The original `docs/02_architecture_overview.md` describes a **future end‚Äëstate** that includes:

- Full LLM integration via **Ollama + Open WebUI**.
- Rich diffusion and video workflows via **ComfyUI + SVD + SadTalker + ESRGAN + Wav2Lip**.
- A complete observability story across all services.
- A more general **LangGraph‚Äëdriven orchestration** on both Node and Python sides.

Phase 2 brings parts of that picture into concrete code while leaving other parts as planned.

### What Now Exists in Code

- **Dual‚Äëplane LangGraph orchestration** is real:
  - Graph JSON is validated on both planes.
  - Control and data executors coordinate via Redis Streams.
  - Capabilities and training use hybrid graphs.
- **Schema‚Äëdriven design** is implemented:
  - Shared schemas ‚Üí AJV validators + Pydantic models.
  - Config and bridge definitions are validated at startup.
- **Job state & progress**:
  - Job lifecycle is fully tracked in Redis.
  - WebSocket updates are driven by mirrored worker Pub/Sub events.
- **Infra & observability**:
  - Prometheus/Grafana/cAdvisor/DCGM are wired in via `worker` and `backend` `/metrics`.

### What Is Still Planned / Stubbed

- **Real GPU workloads**:
  - `train_lora`, `train_voice`, and `render_video` currently simulate work and upload dummy artifacts.
  - ComfyUI, TTS, and lip‚Äësync models are not yet invoked from code.
- **Agentic Planner**:
  - The Planner currently builds a **static default graph** rather than a fully dynamic, LLM‚Äëdriven plan based on capability manifests.
- **End‚Äëto‚Äëend video pipeline**:
  - The sequence _‚ÄúLLM ‚Üí ComfyUI ‚Üí video ‚Üí TTS ‚Üí lip‚Äësync‚Äù_ from the original architecture exists only as a conceptual target, not as a wired pipeline.

---

## 6Ô∏è‚É£ Drift from Initial Design (Architecture Perspective)

This section summarizes how the **current Phase 2 architecture** differs from the original conceptual design, and what remained intact.

### Major Changes vs Original Plan

- **BullMQ removed in favor of LangGraph + Redis Streams**
  - Phase 1 planned BullMQ queues for job orchestration; Phase 2 replaces them with explicit LangGraph graphs and custom Redis Streams consumers on both planes.
- **GPU worker promoted to a first‚Äëclass Data Plane**
  - The worker is no longer a generic ‚ÄúGPU worker‚Äù but a **LangGraph.py executor** with a task registry and metrics, matching the dual‚Äëplane model.
- **Static Planner instead of full agentic planning**
  - The original agentic planning vision exists in `dual_orchestration.md`; the implementation currently uses a fixed template graph for `/api/train` and a small hybrid graph for `/api/capabilities`.
- **Code‚Äëfirst shared schemas**
  - The initial docs mentioned a shared schema approach conceptually; Phase 2 solidifies this into a dedicated `common/` directory, codegen scripts, and validators/models used at runtime.

### Elements Intentionally Preserved

- **End‚Äëgoal topology**
  - The high‚Äëlevel component map (Traefik, UI, API, Redis, MinIO, Prometheus/Grafana, GPU worker, future ComfyUI/LLM/TTS) matches the original architecture and remains the target.
- **Dual‚ÄëLangGraph architecture**
  - The idea that Node handles user‚Äëfacing workflows while Python runs GPU‚Äëheavy DAGs is implemented and will continue to be extended, not replaced.
- **Observability as a first‚Äëclass concern**
  - `/metrics` endpoints, Prometheus scraping, and Grafana dashboards are already present and will be extended as AI pipelines are filled in.

In other words, **Phase 2 delivers the structural skeleton of the intended architecture**‚Äîgraphs, bridges, schemas, and metrics‚Äîwhile leaving model‚Äëheavy components and advanced planning logic for subsequent phases.
